{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529f5590-609a-4554-9254-e22e8a4822ad",
   "metadata": {},
   "source": [
    "# Why parquet?\n",
    "\n",
    "This notebook explores the benefits or drawbacks of using the [parquet](https://parquet.apache.org/docs/) file format relative to other formats such as CSV or SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599a34fd-d468-40ce-aafa-671debbe9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ipyflow reactive mode\n",
    "%flow mode reactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ac8b5f-d7b1-43aa-9589-19890914f646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from IPython.display import Image\n",
    "from utilities import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d7ac6d-3ffa-4a57-99ba-a0214c4e2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target file or table names\n",
    "image_dir = \"images\"\n",
    "csv_name = \"example.csv.gz\"\n",
    "parquet_name = \"example.parquet\"\n",
    "sqlite_name = \"example.sqlite\"\n",
    "sqlite_tbl_name = \"tbl_example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bada16-8022-4125-8083-c937e76d914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any existing prior work\n",
    "for filename in [csv_name, parquet_name, sqlite_name]:\n",
    "    pathlib.Path(filename).unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9eefe2-8379-490c-9977-807b781eb168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataframe_shape (rows, cols)</th>\n",
       "      <th>csv_write_time (secs)</th>\n",
       "      <th>csv_size (bytes)</th>\n",
       "      <th>csv_read_time_all (secs)</th>\n",
       "      <th>csv_read_time_one (secs)</th>\n",
       "      <th>sqlite_write_time (secs)</th>\n",
       "      <th>sqlite_size (bytes)</th>\n",
       "      <th>sqlite_read_time_all (secs)</th>\n",
       "      <th>sqlite_read_time_one (secs)</th>\n",
       "      <th>parquet_write_time (secs)</th>\n",
       "      <th>parquet_size (bytes)</th>\n",
       "      <th>parquet_read_time_all (secs)</th>\n",
       "      <th>parquet_read_time_one (secs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(640, 320)</td>\n",
       "      <td>0.851251</td>\n",
       "      <td>1796821</td>\n",
       "      <td>0.123785</td>\n",
       "      <td>0.050562</td>\n",
       "      <td>0.889879</td>\n",
       "      <td>2654208</td>\n",
       "      <td>0.129853</td>\n",
       "      <td>0.006511</td>\n",
       "      <td>0.185621</td>\n",
       "      <td>2030918</td>\n",
       "      <td>0.040832</td>\n",
       "      <td>0.017435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1280, 640)</td>\n",
       "      <td>3.263002</td>\n",
       "      <td>7176966</td>\n",
       "      <td>0.384910</td>\n",
       "      <td>0.193288</td>\n",
       "      <td>1.394889</td>\n",
       "      <td>7909376</td>\n",
       "      <td>0.403199</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>0.355683</td>\n",
       "      <td>7756901</td>\n",
       "      <td>0.056992</td>\n",
       "      <td>0.028377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2560, 1280)</td>\n",
       "      <td>13.058698</td>\n",
       "      <td>28694773</td>\n",
       "      <td>0.912457</td>\n",
       "      <td>0.656094</td>\n",
       "      <td>2.239445</td>\n",
       "      <td>31543296</td>\n",
       "      <td>1.127243</td>\n",
       "      <td>0.008349</td>\n",
       "      <td>0.791296</td>\n",
       "      <td>30402638</td>\n",
       "      <td>0.128938</td>\n",
       "      <td>0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataframe_shape (rows, cols)  csv_write_time (secs)  csv_size (bytes)  \\\n",
       "0                   (640, 320)               0.851251           1796821   \n",
       "1                  (1280, 640)               3.263002           7176966   \n",
       "2                 (2560, 1280)              13.058698          28694773   \n",
       "\n",
       "   csv_read_time_all (secs)  csv_read_time_one (secs)  \\\n",
       "0                  0.123785                  0.050562   \n",
       "1                  0.384910                  0.193288   \n",
       "2                  0.912457                  0.656094   \n",
       "\n",
       "   sqlite_write_time (secs)  sqlite_size (bytes)  sqlite_read_time_all (secs)  \\\n",
       "0                  0.889879              2654208                     0.129853   \n",
       "1                  1.394889              7909376                     0.403199   \n",
       "2                  2.239445             31543296                     1.127243   \n",
       "\n",
       "   sqlite_read_time_one (secs)  parquet_write_time (secs)  \\\n",
       "0                     0.006511                   0.185621   \n",
       "1                     0.006486                   0.355683   \n",
       "2                     0.008349                   0.791296   \n",
       "\n",
       "   parquet_size (bytes)  parquet_read_time_all (secs)  \\\n",
       "0               2030918                      0.040832   \n",
       "1               7756901                      0.056992   \n",
       "2              30402638                      0.128938   \n",
       "\n",
       "   parquet_read_time_one (secs)  \n",
       "0                      0.017435  \n",
       "1                      0.028377  \n",
       "2                      0.046641  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting rowcount and col count\n",
    "nrows = 320\n",
    "ncols = 160\n",
    "\n",
    "# result list for storing data\n",
    "results = []\n",
    "\n",
    "# loop for iterating over increasingly large dataframes\n",
    "# and gathering data about operations on them\n",
    "for _ in range(1, 4):\n",
    "    # increase the size of the dataframe\n",
    "    nrows *= 2\n",
    "    ncols *= 2\n",
    "\n",
    "    # form a dataframe using randomized data\n",
    "    df = pd.DataFrame(\n",
    "        np.random.rand(nrows, ncols), columns=[f\"col_{num}\" for num in range(0, ncols)]\n",
    "    )\n",
    "\n",
    "    # append data to the result list\n",
    "    results.append(\n",
    "        {\n",
    "            # general information about the dataframe\n",
    "            \"dataframe_shape (rows, cols)\": str(df.shape),\n",
    "            # information about CSV\n",
    "            \"csv_write_time (secs)\": timer(\n",
    "                df.to_csv, path_or_buf=csv_name, compression=\"gzip\"\n",
    "            ),\n",
    "            \"csv_size (bytes)\": os.stat(csv_name).st_size,\n",
    "            \"csv_read_time_all (secs)\": timer(\n",
    "                pd.read_csv, filepath_or_buffer=csv_name, compression=\"gzip\"\n",
    "            ),\n",
    "            \"csv_read_time_one (secs)\": timer(\n",
    "                pd.read_csv,\n",
    "                filepath_or_buffer=csv_name,\n",
    "                compression=\"gzip\",\n",
    "                usecols=[\"col_2\"],\n",
    "            ),\n",
    "            # information about SQLite\n",
    "            \"sqlite_write_time (secs)\": timer(\n",
    "                df.to_sql,\n",
    "                name=sqlite_tbl_name,\n",
    "                con=f\"sqlite:///{sqlite_name}\",\n",
    "            ),\n",
    "            \"sqlite_size (bytes)\": os.stat(sqlite_name).st_size,\n",
    "            \"sqlite_read_time_all (secs)\": timer(\n",
    "                pd.read_sql,\n",
    "                sql=f\"SELECT * FROM {sqlite_tbl_name}\",\n",
    "                con=f\"sqlite:///{sqlite_name}\",\n",
    "            ),\n",
    "            \"sqlite_read_time_one (secs)\": timer(\n",
    "                pd.read_sql,\n",
    "                sql=f\"SELECT col_2 FROM {sqlite_tbl_name}\",\n",
    "                con=f\"sqlite:///{sqlite_name}\",\n",
    "            ),\n",
    "            # information about Parquet\n",
    "            \"parquet_write_time (secs)\": timer(\n",
    "                df.to_parquet, path=parquet_name, compression=\"gzip\"\n",
    "            ),\n",
    "            \"parquet_size (bytes)\": os.stat(parquet_name).st_size,\n",
    "            \"parquet_read_time_all (secs)\": timer(pd.read_parquet, path=parquet_name),\n",
    "            \"parquet_read_time_one (secs)\": timer(\n",
    "                pd.read_parquet, path=parquet_name, columns=[\"col_2\"]\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # remove any existing files in preparation for next steps\n",
    "    for filename in [csv_name, parquet_name, sqlite_name]:\n",
    "        pathlib.Path(filename).unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a735ec26-02d4-42a4-9756-4cee2f33e69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/file-write-time.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write times barchart\n",
    "fig = px.bar(\n",
    "    df_results,\n",
    "    x=[\n",
    "        \"csv_write_time (secs)\",\n",
    "        \"sqlite_write_time (secs)\",\n",
    "        \"parquet_write_time (secs)\",\n",
    "    ],\n",
    "    y=\"dataframe_shape (rows, cols)\",\n",
    "    orientation=\"h\",\n",
    "    barmode=\"group\",\n",
    "    labels={\"dataframe_shape (rows, cols)\": \"Data Shape\", \"value\": \"Seconds\"},\n",
    "    width=1300,\n",
    "    color_discrete_sequence=px.colors.qualitative.D3,\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(x=0.68, y=0.02, bgcolor=\"rgba(255,255,255,0.8)\"),\n",
    "    font=dict(\n",
    "        size=20,  # global font size\n",
    "    ),\n",
    ")\n",
    "\n",
    "pio.write_image(fig, f\"{image_dir}/file-write-time.png\")\n",
    "Image(url=f\"{image_dir}/file-write-time.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4827e5d-b2e4-4b8c-bff8-db96e0b76949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/file-storage-size.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filesize barchart\n",
    "fig = px.bar(\n",
    "    df_results,\n",
    "    x=[\n",
    "        \"csv_size (bytes)\",\n",
    "        \"sqlite_size (bytes)\",\n",
    "        \"parquet_size (bytes)\",\n",
    "    ],\n",
    "    y=\"dataframe_shape (rows, cols)\",\n",
    "    orientation=\"h\",\n",
    "    barmode=\"group\",\n",
    "    labels={\"dataframe_shape (rows, cols)\": \"Data Shape\", \"value\": \"Bytes\"},\n",
    "    width=1300,\n",
    "    color_discrete_sequence=px.colors.qualitative.D3,\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(x=0.72, y=0.02, bgcolor=\"rgba(255,255,255,0.8)\"),\n",
    "    font=dict(\n",
    "        size=20,  # global font size\n",
    "    ),\n",
    ")\n",
    "\n",
    "pio.write_image(fig, f\"{image_dir}/file-storage-size.png\")\n",
    "Image(url=f\"{image_dir}/file-storage-size.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab638a5-e6e1-47e5-a5f9-56f156a71bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/file-read-time-all.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read time barchart (all columns)\n",
    "fig = px.line(\n",
    "    df_results,\n",
    "    y=[\n",
    "        \"csv_read_time_all (secs)\",\n",
    "        \"sqlite_read_time_all (secs)\",\n",
    "        \"parquet_read_time_all (secs)\",\n",
    "    ],\n",
    "    x=\"dataframe_shape (rows, cols)\",\n",
    "    labels={\"dataframe_shape (rows, cols)\": \"Data Shape\", \"value\": \"Seconds\"},\n",
    "    width=1300,\n",
    "    color_discrete_sequence=px.colors.qualitative.D3,\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(x=0.01, y=0.98, bgcolor=\"rgba(255,255,255,0.8)\"),\n",
    "    font=dict(\n",
    "        size=20,  # global font size\n",
    "    ),\n",
    ")\n",
    "fig.update_xaxes(range=[0, 2.13])\n",
    "fig.update_traces(mode=\"lines+markers\")\n",
    "\n",
    "pio.write_image(fig, f\"{image_dir}/file-read-time-all.png\")\n",
    "Image(url=f\"{image_dir}/file-read-time-all.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd99f232-8564-4b83-82c7-e4b0cc5cfbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/file-read-time-one.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read time barchart (one column)\n",
    "fig = px.bar(\n",
    "    df_results,\n",
    "    x=[\n",
    "        \"csv_read_time_one (secs)\",\n",
    "        \"sqlite_read_time_one (secs)\",\n",
    "        \"parquet_read_time_one (secs)\",\n",
    "    ],\n",
    "    y=\"dataframe_shape (rows, cols)\",\n",
    "    orientation=\"h\",\n",
    "    barmode=\"group\",\n",
    "    labels={\"dataframe_shape (rows, cols)\": \"Data Shape\", \"value\": \"Seconds\"},\n",
    "    width=1300,\n",
    "    color_discrete_sequence=px.colors.qualitative.D3,\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(x=0.65, y=0.02, bgcolor=\"rgba(255,255,255,0.8)\"),\n",
    "    font=dict(\n",
    "        size=20,  # global font size\n",
    "    ),\n",
    ")\n",
    "\n",
    "pio.write_image(fig, f\"{image_dir}/file-read-time-one.png\")\n",
    "Image(url=f\"{image_dir}/file-read-time-one.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipyflow)",
   "language": "python",
   "name": "ipyflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
